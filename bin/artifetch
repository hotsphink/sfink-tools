#!/usr/bin/python

# [ ] When recording, there are a number of duplicate recorded files. Are we
#     re-fetching the same data multiple times?

import argparse
import gzip
import io
import json
import logging
import os
import re
import requests
import sys
import urllib
import yaml

from collections import defaultdict
from pathlib import Path
from typing import Any

DEFAULT_CACHE_ROOT = Path(os.environ.get("XDG_CACHE_HOME", os.path.expanduser("~/.cache")))
TREEHERDER_SERVER = "https://treeherder.mozilla.org"
VERSION = 0.1
HEADERS = {"User-Agent": f"artifetch {VERSION} by sfink@mozilla.com"}

PUSH_URL = "https://treeherder.mozilla.org/jobs?repo={repo}&revision={revision}"
JOB_URL = (
    "https://treeherder.mozilla.org/jobs?repo={repo}&revision={revision}&selectedTaskRun={task_id}"
)

DEFAULT_QUERY = {
    "artifact": "live_backing.log",
    "metric": {
        "output": {
            "style": "formatted",
            "push-header": "# from push {push_id} - {push_desc}",
            "format": "{filename}",
        }
    },
}

logger = logging.getLogger("artifetch")

if "--test" in sys.argv:
    iftest = lambda t: t
else:
    iftest = lambda t: argparse.SUPPRESS

parser = argparse.ArgumentParser(prog="artifetch")
parser.add_argument("--version", action="version", version=f"%(prog)s {VERSION}")
parser.add_argument("--verbose", "-v", default=0, action="count", help="Verbose output")

g_input = parser.add_argument_group(title="Specifying input")
g_input.add_argument("--user", "-u", default=None, help="User to use for filtering pushes")
g_input.add_argument(
    "--branch", default="try", help="Treeherder branch (eg 'try', 'mozilla-central')"
)
g_input.add_argument(
    "--artifacts",
    metavar="PATTERN",
    help="Restrict artifacts to those matching PATTERN",
)
g_input.add_argument(
    "--pushes",
    help="List of push IDs to query, or !n for the nth newest history item, counting from zero",
)

g_output = parser.add_argument_group(title="Output")
g_output.add_argument(
    "--json",
    action="store_true",
    help="Dump out raw JSON output (only valid for --list-jobs, --list-pushes, --show-job)",
)
g_output.add_argument("--output", "-o", help="File to save output to (default stdout)")

g_action = parser.add_argument_group(title="Actions")
g_action.add_argument(
    "--again",
    nargs="?",
    const="all",
    default="",
    action="store",
    metavar="INDEX",
    help="Repeat last query",
)
g_action.add_argument(
    "--list-pushes",
    nargs="?",
    const=10,
    action="store",
    metavar="N",
    help="List out N most recent try pushes",
)
g_action.add_argument(
    "--list-jobs",
    nargs="?",
    action="store",
    const=True,
    metavar="MATCH",
    help="List out jobs for a push",
)
g_action.add_argument(
    "--list-artifacts",
    nargs="?",
    const="//",
    action="store",
    metavar="MATCH",
    help="List out artifacts (default: all)",
)
g_action.add_argument("--show-job", dest="job", help="Display info for a given JOB id")
g_action.add_argument("--query", help="Query YAML file (may also be given as a plain argument)")
g_action.add_argument("_query", nargs="?", default=None, help=argparse.SUPPRESS)

g_test = parser.add_argument_group(title="Testing")

g_test.add_argument("--test", action="store_true", help="enable extra options for testing")
g_test.add_argument("--record", type=str, help=iftest("directory to record into"))
g_test.add_argument("--replay", type=str, help=iftest("directory to replay from"))

g_cache = parser.add_argument_group(title="Cache control")
g_cache.add_argument(
    "--cache-root",
    default=str(DEFAULT_CACHE_ROOT),
    help=f"Root of cache dir (default: {DEFAULT_CACHE_ROOT})",
)
g_cache.add_argument("--refresh", action="store_true", help="Do not use cached results")
g_cache.add_argument("--no-cache", action="store_true", help="Do not load from or save to cache")


args = parser.parse_args()

if args.query is None:
    args.query = args._query
if args.query != args._query and args._query is not None:
    print("Do not give both --query and positional query", file=sys.stderr)
    sys.exit(1)

if args.verbose == 1:
    logger.setLevel(logging.INFO)
elif args.verbose > 1:
    logger.setLevel(logging.DEBUG)

loghandler = logging.StreamHandler()
loghandler.setFormatter(logging.Formatter("%(levelname)s - %(message)s"))
logger.addHandler(loghandler)

OutFile = sys.stdout
if args.output:
    OutFile = open(args.output, "wt")

if args.again == "all":
    args.again = ["pushes", "jobs"]
else:
    args.again = args.again.split(",")


class Cache(object):
    # Anything here can be guaranteed to exist.
    EMPTY_CACHE = {
        "pushes": {
            # "spec=<spec>" for resolve_push_range a+b::c+d+e syntax (b::c ranges are looked up).
        },
        "job": {
            # <job_id>: { "artifacts": list of artifact urls }
        },
        "push": {
            # <push_id>: { "desc": description of push grabbed from first revision }
        },
    }

    def __init__(self, cache_root, refresh, no_cache):
        self.path = Path(cache_root) / "artifetch/cache.json"
        self.refresh = refresh
        self.no_cache = no_cache
        self.data = None

    def load(self):
        if self.refresh or self.no_cache:
            self.data = self.EMPTY_CACHE
            return

        try:
            with open(self.path, "rt") as fh:
                self.data = json.load(fh)
        except OSError:
            self.data = self.EMPTY_CACHE

    def save(self):
        if self.data is None:
            logger.info("no cache data to save")
            return
        if self.no_cache:
            logger.info("--no-cache given, not saving")
            return
        os.makedirs(os.path.dirname(self.path), exist_ok=True)
        with open(self.path, "wt") as fh:
            json.dump(self.data, fh)
        logger.info(f"saved to cache file {self.path}")

    def lookup(self, path):
        cache = self.data
        if cache is None:
            return None
        for p in path:
            if isinstance(cache, list):
                i = int(p)
                next = cache[i] if i < len(cache) else None
            else:
                next = cache.get(str(p))
            if next is None:
                return None
            cache = next
        logger.debug(f"loaded {'.'.join(str(p) for p in path)} from cache")
        return cache

    def value(self, path, value):
        """Insert a value at a path in the cache, and return the value."""
        cache = self.data
        if cache:
            for segment in path[:-1]:
                cache = cache.setdefault(str(segment), {})
            cache[path[-1]] = value
        return value


def require_user():
    if args.user is not None:
        if args.user.lower() in ("any", ""):
            return None
        return args.user
    import subprocess

    try:
        output = subprocess.check_output(["hg", "config", "ui.username"], text=True)
        # Extract just the email address from `My Name <myname@nowhere.com>`.
        if m := re.search(r"<(.*?)>", output):
            args.user = m.group(1)
        else:
            args.user = output.strip()
    except subprocess.CalledProcessError:
        output = subprocess.check_output(["git", "config", "--get", "user.email"], text=True)
        args.user = output.strip()

    return args.user


class TreeHerder(object):
    def __init__(self, cache_root):
        self.server = TREEHERDER_SERVER
        self.headers = HEADERS
        self.artifact_path = Path(cache_root) / "artifetch/artifacts/"

    # I should be using thclient, but the pip-installed version is out of date, and
    # the in-tree treeherder one is missing some endpoints that I need. And it
    # doesn't really add much portability or backwards compatibility afaict.
    def get(self, endpoint, project=None, **params):
        if project is not None:
            url = f"{self.server}/api/project/{project}/{endpoint}/"
        else:
            url = f"{self.server}/api/{endpoint}/"
        return requests.get(url, params=params, headers=self.headers)

    def get_results(self, endpoint, project=None, **params):
        response = self.get(endpoint, project=project, **params)
        if not response.ok:
            logger.error(f"API request failed: {response.status_code} {response.reason}")
            logger.error(response.text)
            sys.exit(1)
        data = response.json()

        return data["results"] if endpoint != "repository" else data

    def graphql(self, task_id):
        headers = self.headers
        headers.update({"Content-Type": "application/json"})
        response = requests.post(
            "https://firefox-ci-tc.services.mozilla.com/graphql",
            json={
                "operationName": "Task",
                "variables": {
                    "taskId": task_id,
                    "artifactsConnection": {"limit": 1000},
                },
                "query": """\
    query Task(
        $taskId: ID!,
        $artifactsConnection: PageConnection
    ) {
      task(taskId: $taskId) {
        taskId
        taskGroupId
        routes
        extra
        metadata {
          name
          description
          owner
          __typename
        }
        status {
          state
          runs {
            taskId
            runId
            state
            artifacts(connection: $artifactsConnection) {
              ...Artifacts
              __typename
            }
            __typename
          }
          __typename
        }
        __typename
      }
    }

    fragment Artifacts on ArtifactsConnection {
      pageInfo {
        hasNextPage
        hasPreviousPage
        cursor
        previousCursor
        nextCursor
        __typename
      }
      edges {
        node {
          name
          contentType
          __typename
        }
        __typename
      }
      __typename
    }""",
            },
            headers=headers,
        )

        if not response.ok:
            logger.error(f"graphql request failed: {response.status_code} {response.reason}")
            logger.error(response.text)
            sys.exit(1)
        data = response.json()
        if errors := data.get("errors"):
            for error in errors:
                logger.error("{code}: {message}".format(**error))
            raise Exception(",".join(e["code"] for e in errors))
        return data

    def fetch_artifact(self, url):
        if url.startswith("file://"):
            # requests doesn't seem to handle file: URLs.
            filename = url[7:]
            return (Path(filename).read_text(), filename)

        cache_file_base = self.artifact_path / urllib.parse.quote(url, safe="")
        data = None
        for cache_file in (cache_file_base, Path(str(cache_file_base) + ".gz")):
            try:
                with gzip.open(cache_file.open("rb"), "rt") as fh:
                    data = fh.read()
            except gzip.BadGzipFile:
                data = cache_file.read_text()
            except OSError:
                pass

            if data is not None:
                break

        if data is not None and len(data):
            logger.info(f"used cached file {cache_file}")
            return (data, cache_file)

        # Hack: read full file into memory in order to handle both compressed and
        # uncompressed data.
        r = requests.get(url, stream=True)
        raw_data = r.raw.read()

        # Further hacks: the URL may not have a .gz extension even though the
        # data is compressed. Adjust the cache_file name to reflect the compression or lack thereof.

        self.artifact_path.mkdir(parents=True, exist_ok=True)

        try:
            data = gzip.GzipFile(fileobj=io.BytesIO(raw_data)).read().decode()
            if not str(cache_file).endswith(".gz"):
                cache_file = cache_file.with_name(cache_file.name + ".gz")
            cache_file.write_bytes(raw_data)
        except gzip.BadGzipFile:
            data = raw_data.decode()
            cache_file.write_text(data)

        return (data, cache_file)


# I'm sure there are many better solutions for this already. I wrote this as a
# learning project to better understand the space.
#
# One particularly weird thing is that if you have the same query multiple
# times, this won't even notice. If replaying, it will always use the last
# result. If recording, it will create a new file and overwrite the mapping
# table with the latest result.
#
# I probably ought to do something about that. This is a bug even if the
# results are deterministic, because if during replay you request more entries
# than during the original recording, the last one will be reused.
#
# Note that this allows replaying from one directory and recording into
# another. This allows extending partial recordings: part of the run is
# replayed, then the rest will do actual fetches. When replaying without
# recording, missing keys will error.
class Recorder(object):
    def __init__(self, replay_dir, record_dir):
        self.replay_dir = Path(replay_dir) if replay_dir else None
        self.record_dir = Path(record_dir) if record_dir else None
        self.autoincrement = defaultdict(int)
        self.mapping = self.load()

    def load(self):
        if not self.replay_dir:
            return {}

        try:
            data = json.loads((self.replay_dir / "mapping.json").read_text())
            return {tuple(dict(key).items()): v for key, v in data}
        except IOError:
            return {}

    def save(self):
        if self.record_dir:
            mapping = [[list(k), v] for k, v in self.mapping.items()]
            (self.record_dir / "mapping.json").write_text(json.dumps(mapping))

    def get_filename(self, params):
        endpoint = params["endpoint"]
        self.autoincrement[endpoint] += 1

        key = tuple(params.items())  # Does not recurse!
        if self.replay_dir:
            try:
                return self.mapping[key]
            except KeyError:
                if self.record_dir:
                    logger.info("Extending a replay with new data")
                else:
                    logger.error(
                        "Replay diverged! Requested a key that does not exist in the recording."
                    )
                    logger.error(json.dumps(params))
                    raise

        filename = endpoint + "-" + str(self.autoincrement[endpoint])
        self.mapping[key] = filename
        return filename

    def record(self, filename, data, format):
        if format == "json":
            data = json.dumps(data)
        (self.record_dir / filename).write_text(data)

    def replay(self, filename, format):
        text = (self.replay_dir / filename).read_text()
        if format == "json":
            return json.loads(text)
        return text


class PersistentTreeHerder(TreeHerder):
    def __init__(self, cache_root, replay, record):
        super().__init__(cache_root)
        self.replay = replay
        self.record = record
        self.recorder = Recorder(self.replay, self.record)

    def do(self, key, func, format):
        if not self.replay and not self.record:
            return func()

        filename = self.recorder.get_filename(key)

        if self.replay:
            data = self.recorder.replay(filename, format)
        else:
            data = func()

        if self.record:
            self.recorder.record(filename, data, format)

        return data

    def get_results(self, endpoint, project=None, **params):
        sup = super()
        key = dict(params)
        key.update({"endpoint": endpoint, "project": project})
        return self.do(
            key,
            lambda: sup.get_results(endpoint, project=project, **params),
            format="json",
        )

    # We only have a single graphql query, hence the stupid name here.
    def graphql(self, task_id):
        sup = super()
        key = {"endpoint": "graphql", "task_id": task_id}
        return self.do(key, func=lambda: sup.graphql(task_id), format="json")

    def fetch_artifact(self, url):
        sup = super()
        cache_file = None

        # fetch_artifact returns the data along with the Path to the data file,
        # which is not JSON serializable, so strip it out for self.do() and add
        # it back when returning.
        def call_super():
            nonlocal cache_file
            (data, cache_file) = sup.fetch_artifact(url)
            return data

        key = {"endpoint": "artifact", "url": url}
        data = self.do(key, call_super, "text")
        return (data, cache_file)


server = PersistentTreeHerder(args.cache_root, args.replay, args.record)

# Global variable because I am bad.
PushDescriptions = {}


# Summarize and cache a push.
def summarize_push(push, cache):
    cache_path = ("push", push["id"])
    if desc := cache.lookup(cache_path + ("desc",)):
        return desc

    rev = push["revisions"][0]
    desc = re.sub(r"\n+\s*", " ; ", rev["comments"])
    if len(desc) >= 76:
        desc = desc[0:76] + "..."
    desc = f"{push['revision'][0:12]} - {desc}"
    PushDescriptions[push["id"]] = desc
    push["desc"] = desc
    cache.value(cache_path, push)
    return desc


def list_pushes(args):
    user = require_user()
    params = {
        "project": args.branch,
        "count": args.list_pushes,
    }
    if user is not None:
        params["user"] = user
    pushes = server.get_results("push", **params)
    if args.json:
        print(json.dumps(pushes, indent=4), file=OutFile)
        return

    for p in pushes:
        print(f"{p['id']} - {len(p['revisions'])} revs", file=OutFile)
        for rev in p["revisions"]:
            desc = re.sub(r"\n.*", "", rev["comments"])
            print(f"  {rev['revision']} {desc}", file=OutFile)


def get_push_summary(push_id, cache):
    if desc := cache.lookup(("push", push_id, "desc")):
        return desc
    push = server.get_results("push", project=args.branch, id=push_id)[0]
    return summarize_push(push, cache)


def get_push(push_id, cache):
    if push := cache.lookup(("push", push_id)):
        return push
    push = server.get_results("push", project=args.branch, id=push_id)[0]
    summarize_push(push, cache)
    return push


def choose_pushes(cache, n=10, single=False):
    user = require_user()
    pushes = server.get_results("push", project=args.branch, author=user, count=n)
    push_filter = ChooseFilter(
        None,
        cache,
        "pushes" in args.again,
        single=single,
        extract_description=lambda p: summarize_push(p, cache),
    )
    return [push["id"] for push in push_filter(pushes)]


def choose_push(cache, n=10):
    return choose_pushes(cache, n, single=True)[0]


def get_jobs(push_id):
    offset = 0
    while True:
        chunk = server.get_results(
            "jobs", project=args.branch, push_id=push_id, offset=offset, count=1000
        )
        offset += len(chunk)
        for job in chunk:
            yield job
        if len(chunk) < 1000:
            break


def list_jobs(args, push_id, job_pattern, cache):
    if job_pattern is True:
        job_query = {}
    else:
        job_query = {"symbol": job_pattern}
    job_filter = make_job_filter(job_query, cache)
    jobs = job_filter(get_jobs(push_id))

    if args.json:
        print(json.dumps(list(jobs), indent=4), file=OutFile)
        return

    for j in jobs:
        print(describe_job(j, {"id": True}))


def job_symbol(job):
    if job is None:
        return "?"
    if job["job_group_symbol"] == "?":
        return job["job_type_symbol"]
    return "{job_group_symbol}({job_type_symbol})".format(**job)


def describe_job_id(job_id, cache):
    job = cache.lookup(("job", job_id))
    return job_symbol(job)


def describe_job(job, detail={"id": False}):
    template = "{job_symbol} - {job_type_name} ({state}: {result})"
    if detail.get("id"):
        template = "{id} " + template
    return template.format(**job, job_symbol=job_symbol(job))


def get_job_artifacts(job_id, cache):
    if cjob := cache.lookup(("job", job_id)):
        if artifacts := cjob.get("artifacts"):
            if "job_group_symbol" in cjob:
                logger.info(
                    f"loaded job {job_id} {describe_job_id(job_id, cache)} for push {cjob['push_id']} from cache: {len(artifacts)} artifacts"
                )
                return artifacts

    job = server.get_results("jobs", project=args.branch, id=job_id)[-1]
    for k in job.keys():
        cache.value(("job", job_id, k), job[k])

    task = job["task_id"]
    data = server.graphql(task)["data"]
    runs = data["task"]["status"]["runs"]
    lastrun = runs[-1]
    edges = lastrun["artifacts"]["edges"]
    names = [e["node"]["name"] for e in edges]
    artifacts = []
    for name in names:
        artifacts.append(
            "{site}/api/queue/v1/task/{task}/runs/{run}/artifacts/{artifact}".format(
                site="https://firefox-ci-tc.services.mozilla.com",
                task=task,
                run=lastrun["runId"],
                artifact=urllib.parse.quote(name, safe=""),
            )
        )
    cache.value(("job", job_id, "artifacts"), artifacts)
    logger.debug(f"job {job_id} {describe_job_id(job_id, cache)}: {len(artifacts)} artifacts")
    return artifacts


def list_artifacts(args, cache):
    if args.pushes is None:
        push = choose_push(cache, 20)
    else:
        push = args.pushes[0]
    match = args.list_artifacts

    job_filter = make_job_filter({"choose-from": 0}, cache)
    for job in job_filter(get_jobs(push)):
        logger.debug(f"Fetching artifacts for push {push} job {job['id']}")
        for a in get_job_artifacts(job["id"], cache):
            if match_string(a, match):
                print(a, file=OutFile)


def show_job(args):
    if not args.job:
        raise Exception("--job ID required")

    job = server.get_results("jobs", project=args.branch, id=args.job)

    if args.json:
        print(json.dumps(job, indent=4), file=OutFile)


def get_pushes(start, end):
    result = []

    user = require_user()
    pushes = server.get_results("push", project=args.branch, author=user, count=50)
    for p in reversed(pushes):
        id = p["id"]
        if id == start:
            result.append(id)
        elif len(result):
            result.append(id)
            if id == end:
                return result

    recent = [p["id"] for p in pushes]
    raise Exception(f"do not see {start}::{end} in recent pushes {recent}")


def get_pushes_matching(filter):
    user = require_user()
    pushes = server.get_results("push", project=args.branch, author=user, count=20)
    return [p["id"] for p in reversed(pushes) if filter(p)]


def resolve_push_range(spec, cache):
    cache_path = ("pushes", f"spec={spec}")
    if pushes := cache.lookup(cache_path):
        return pushes

    pushes = []
    for part in spec.split("+"):
        range = part.split("::")
        if len(range) == 1:
            pushes.append(part)
        else:
            pushes.extend(get_pushes(int(range[0]), int(range[1])))

    return cache.value(cache_path, pushes)


def resolve_pushes(spec, cache, single=False):
    if args.pushes:
        if args.pushes.startswith("!"):
            histdepth = int(args.pushes[1:])
            spec = {"ids": "+".join(cache.lookup(("history", "pushes", -histdepth)))}
        else:
            spec = {"ids": args.pushes}

    if ids := spec.get("ids"):
        return resolve_push_range(ids, cache)

    if comment := spec.get("comment"):
        # Do not cache, because we may be rerunning to pick up newer pushes.
        def push_has_comment(p):
            return any([comment in r["comments"] for r in p["revisions"]])

        return get_pushes_matching(filter=push_has_comment)

    if revision := spec.get("rev"):
        # Do not cache, because we may be rerunning to pick up newer pushes.
        def rev_matches(rev, pattern):
            return rev in pattern.split("+")

        def push_has_rev(p):
            return any([rev_matches(r["revision"], revision) for r in p["revisions"]])

        return get_pushes_matching(filter=push_has_rev)

    if nchoices := spec.get("choose-from"):
        return choose_pushes(cache, nchoices)

    raise Exception("pushes request must be 'ids' or 'comment'")


DataT = dict[str, Any] | list[Any]
PathT = list[str | int]
ResultPathsT = list[PathT]


def parse_path(k: str) -> PathT:
    return [p for p in re.findall(r"\[\]|[^.\[]+|\.", k) if p != "."]


def find_path_matches(data: DataT, path: PathT, sofar: PathT = []) -> list[tuple[PathT, Any]]:
    if not path:
        return [(sofar, data)]
    results = []
    if path[0] == "[]":
        for i in range(len(data)):
            results += find_path_matches(data[i], path[1:], sofar + [i])
    else:
        k = path[0]
        if isinstance(data, list):
            k = int(k)
        results += find_path_matches(data[k], path[1:], sofar + [k])
    return results


def resolve_path(path: PathT, bases: ResultPathsT):
    result = []
    for segment in path:
        if segment.startswith("$"):
            i = int(segment[1:] or "1")
            # $n converts to the *parent* of match-key-n.
            result.extend(bases[i][:-1])
        elif segment == "__parent__":
            result.pop()
        else:
            result.append(segment)
    return result


def lookup(path: PathT, data):
    for segment in path:
        data = data[segment]
    return data


def lookup_json_values_g(extractor, match: ResultPathsT, data, labels):
    path = parse_path(extractor["values"])
    path = resolve_path(path, match)

    for p, datum in find_path_matches(data, path):
        result = {"value": datum}
        for lname, lpath in labels:
            result[lname] = lookup(resolve_path(lpath, match), data)
        yield result


def lookup_text_values_g(extractor, data, labels):
    pattern = extractor["keys"][1]
    assert pattern.startswith("/") and pattern.endswith("/")
    matcher = re.compile(pattern[1:-1])
    for line in data.splitlines():
        if m := re.search(matcher, line):
            result = {}
            for lname, lvalue in [("value", extractor["values"])] + labels:
                result[lname] = re.sub(r"\$(\d+)", lambda lm: m.group(int(lm.group(1))), lvalue)
            yield result


def lookup_values_g(extractor, match, data, labels):
    if extractor["type"] == "json":
        yield from lookup_json_values_g(extractor, match, data, labels)
    elif extractor["type"] == "text":
        yield from lookup_text_values_g(extractor, data, labels)
    elif extractor["type"] == "files":
        yield {}


def match_string(input, selector):
    if selector.startswith("/") and selector.endswith("/"):
        matcher = re.compile(selector[1:-1])
        return matcher.search(input)
    return input == selector


def match_key(key: str, val):
    if key.startswith("/"):
        return bool(re.search(key[1:-1], val))
    else:
        val = re.sub(r".*%2F", "", val)
        return key == val


def get_repository(repo_id, cache):
    repo = cache.lookup(("repository", repo_id))
    if repo:
        return repo

    results = server.get_results("repository")
    cached = cache.value(("repository",), {})
    retrieved = {str(r["id"]): r["name"] for r in results}
    cached.update(retrieved)
    return cached[str(repo_id)]


def output_metric(result, added, output={}, cache=None):
    global PushDescriptions

    if not hasattr(output_metric, "PreviousOutput"):
        output_metric.PreviousOutput = {}

    style = output.get("style", "formatted")
    format = output.get("format", "{push_idx} {value}")
    job_header = output.get("job-header", "# {desc}" if "desc" in result else False)
    push_header = output.get("push-header", False)

    if style not in ("gnuplot", "formatted"):
        raise Exception(f"unsupported output format {output}")

    if push_header is not False:
        prev_push = output_metric.PreviousOutput.get("push")
        if result["push_id"] != prev_push:
            if prev_push is not None:
                print("", file=OutFile)
            print(push_header.format(**result), file=OutFile)
        output_metric.PreviousOutput["push"] = result["push_id"]

    if job_header is not False:
        prev_job = output_metric.PreviousOutput.get("job")
        if result["job_id"] != prev_job:
            if prev_job is not None:
                print("", file=OutFile)
            print(job_header.format(**result), file=OutFile)
        output_metric.PreviousOutput["job"] = result["job_id"]

    # Every time a new index is generated, output the label-header format so the index can be mapped to the data it corresponds to.
    for label, idx, label_value in added:
        if label_format := output.get("label-header", {}).get(label):
            print(label_format.format(**result), file=OutFile)

    print(format.format(**result), file=OutFile)


def parse_json_metric_extractor(metric):
    keys = [None]
    targets = [None]

    # TODO: Error checking. (This assumes the keys are well-named and
    # corresponding.)

    json = metric["json"]

    if mk1 := json.get("match-key"):
        keys.append(parse_path(mk1))
        targets.append(json["match-value"])
    else:
        for k in json.keys():
            parts = k.split("match-key-", 1)
            if len(parts) == 1:
                continue
            i = int(parts[1])
            while len(keys) <= i:
                keys.append(None)
                targets.append(None)
            keys[i] = parse_path(json[f"match-key-{i}"])
            targets[i] = json[f"match-value-{i}"]

    return {
        "type": "json",
        "keys": keys,
        "targets": targets,
        "values": json["value"],
        "labels": json.get("label"),
    }


def parse_text_metric_extractor(metric):
    spec = metric["text"]
    return {
        "type": "text",
        "keys": (
            None,
            spec["match-key"],
        ),
        "targets": (
            None,
            True,
        ),
        "values": spec["value"],
        "labels": spec.get("label", ()),
    }


def parse_metric_extractor(metric):
    if "json" in metric:
        return parse_json_metric_extractor(metric)
    if "text" in metric:
        return parse_text_metric_extractor(metric)
    return {
        "type": "files",
        "labels": {},
        "keys": (None,),
        "targets": (None,),
    }


def match_remaining(matchers, paths: ResultPathsT, data: DataT) -> list[ResultPathsT]:
    if not matchers:
        return [paths]

    i, keypath, target = matchers[0]
    keypath = resolve_path(keypath, paths)

    result: list[ResultPathsT] = []
    for path, keyval in find_path_matches(data, keypath):
        if match_key(target, keyval):
            result.append(paths + [path])
    return result


def extract_matches_g(extractor, data: DataT):
    matchers = list(zip(range(len(extractor["keys"])), extractor["keys"], extractor["targets"]))
    matchers.pop(0)  # Matchers are 1-based indexes. Sorry.

    matches: list[ResultPathsT] = []

    if extractor["type"] == "files":
        yield from lookup_values_g(extractor, None, data, {})
        return

    # For each match for the first match-key
    matcher = matchers.pop(0)

    if extractor["type"] == "json":
        labels = []
        for label, path in (extractor["labels"] or {}).items():
            labels.append((label, parse_path(path)))

        for path, keyval in find_path_matches(data, matcher[1]):
            if match_key(matcher[2], keyval):
                # Find everything that matches the rest.
                matches += match_remaining(matchers, [None, path], data)

        for match in matches:
            yield from lookup_values_g(extractor, match, data, labels)

    else:
        labels = list((extractor["labels"] or {}).items())
        yield from lookup_values_g(extractor, None, data, labels)


class GroupBy(object):
    def __init__(self, groupby, expr, cache):
        self.groupby = groupby
        self.expr = expr
        self.cache = cache
        self.aggregates = []
        for m in re.finditer(r"{(\w+)\((\w+)\)}", expr):
            self.aggregates.append((m.group(1), m.group(2)))
        self.sums = defaultdict(lambda: defaultdict(int))
        self.counts = defaultdict(int)
        self.proto_result = {}

        self.job = {}
        self.filename = {}
        self.output = None

    def __call__(self, result, added, output, cache):
        rawkey = (result[k] for k in self.groupby)
        # Allow `groupby: ["push"]`` for example (objects will be distinguished by their "id" values.)
        key = tuple(k["id"] if isinstance(k, dict) else k for k in rawkey)
        for func, field in self.aggregates:
            self.sums[key][field] += int(result[field])
            self.counts[key] += 1
        self.proto_result.setdefault(key, {}).update(result)

    def output_results(self, output):
        for key in self.sums.keys():
            # This is a complete mess.
            result = dict(self.proto_result[key])
            for m in re.finditer(r"{(\w+)\((\w+)\)}", self.expr):
                func, field = m.groups()
                call = m.group(0)[1:-1]
                if func == "sum":
                    result[call] = self.sums[key][field]
                elif func == "count":
                    result[call] = self.counts[key]
                elif func == "mean":
                    result[call] = self.sums[key][field] / self.counts[key]
                else:
                    raise Exception(f"unknown aggregate function '{func}'")

            output_metric(result, (), output, self.cache)


def process_artifact(url, job, push_result, query, cache):
    logger.info(f"process artifact {url}")
    raw, filename = server.fetch_artifact(url)
    metric = query["metric"]
    extractor = parse_metric_extractor(metric)
    if extractor["type"] == "json":
        data: DataT = json.loads(raw)
    elif extractor["type"] == "text":
        data: DataT = raw
    else:
        data: DataT = None
    output = metric.get("output", {})

    if groupby := output.get("groupby"):
        handler = GroupBy(groupby, output["format"], cache)
    else:
        handler = output_metric

    # {label name: {value: idx}}
    labeled_values = defaultdict(dict)

    job_result = dict(push_result)
    job_result.update(
        {
            "filename": filename,
            "job": job,
            "job_id": job["id"],
            "job_desc": describe_job_id(job["id"], cache),
            "task_id": job["task_id"],
        }
    )
    job_result["job_url"] = JOB_URL.format(**job_result)

    indexes = {}
    for result in extract_matches_g(extractor, data):
        result.update(job_result)
        added = []
        for label, value in result.items():
            if isinstance(value, dict):
                # eg job will be skipped. job_idx will be based off of job_id.
                continue
            if label.endswith("_idx"):
                # don't need indexes of indexes
                continue
            if label.endswith("_id"):
                # eg job_id -> job, which will then generate job_idx
                label = label[:-3]

            if f"{label}_idx" in result:
                continue  # eg push_idx is a global index; do not override.
            map = labeled_values[label]
            if (idx := map.get(value)) is None:
                idx = len(map)
                added.append((label, idx, value))
                map[value] = idx

            indexes[f"{label}_idx"] = idx

        result.update(indexes)
        handler(result, added, output, cache)

    if output.get("groupby"):
        handler.output_results(output)


class BaseFilter(object):
    def __init__(self, base_filter=None, desc=None):
        self.base = base_filter
        if desc is None:
            self.name = base_filter.name if base_filter else "all items"
        else:
            self.name = f"all {desc}"

    def __call__(self, incoming):
        if self.base is None:
            return incoming
        return self.base(incoming)


class LimitFilter(BaseFilter):
    def __init__(self, base_filter, limit):
        super().__init__(base_filter)
        self.limit = limit
        if self.limit:
            self.name = f"first {self.limit} of {base_filter.name}"
        else:
            self.name = base_filter.name

    def __call__(self, incoming):
        results = []
        for item in super().__call__(incoming):
            results.append(item)
            if self.limit and len(results) >= self.limit:
                return results
        return results


class MatchFilter(BaseFilter):
    def __init__(self, base_filter, matcher, desc):
        super().__init__(base_filter)
        self.name = f"({base_filter.name} with {desc}"
        self.matcher = matcher

    def __call__(self, incoming):
        return filter(self.matcher, super().__call__(incoming))


class ChooseFilter(BaseFilter):
    def __init__(self, base_filter, cache, again, single=False, extract_description=lambda s: s):
        super().__init__(base_filter)
        self.extract_description = extract_description
        self.cache = cache
        self.again = again
        self.single = single
        setattr(ChooseFilter, "count", 0)

    def __call__(self, incoming):
        incoming = list(super().__call__(incoming))  # flatten iterator

        # TODO: history stores just the indexes of the choices. It should really give the id
        # or something persistent about the selections.
        ChooseFilter.count += 1
        n = ChooseFilter.count
        # TODO: key these with the type of history item. This could easily mix things up. It
        # should also follow the hierarchy, probably (so job history would be specific to the
        # relevant push.)
        cache_path = ("history", "choose", f"choose-{n}")

        indexes = None
        if self.again:
            previous = self.cache.lookup(cache_path)
            if previous is not None:
                indexes = [int(i) for i in previous]

        if indexes is None:
            import pyfzf

            fzf = pyfzf.FzfPrompt()
            descriptions = map(self.extract_description, incoming)
            ids = range(1, len(incoming) + 1)
            options = [f"{id} - {desc}" for id, desc in zip(ids, descriptions)]
            cmd_args = "--no-sort --marker=* --ansi"
            if not self.single:
                cmd_args += " --multi --bind=ctrl-a:select-all"
            choices = fzf.prompt(options, cmd_args)
            indexes = [int(c[0 : c.index(" ")]) - 1 for c in choices]
            self.cache.value(cache_path, indexes)

        return [incoming[i] for i in indexes]


def make_job_filter(job_query, cache):
    job_filter = BaseFilter(None, "jobs")

    if sym := job_query.get("symbol"):
        job_filter = MatchFilter(
            job_filter, lambda j: match_string(job_symbol(j), sym), f"symbol is {sym}"
        )
    if name := job_query.get("group_name"):
        job_filter = MatchFilter(
            job_filter,
            lambda j: match_string(j["job_group_name"], name),
            f"group name is {name}",
        )
    state = job_query.get("state", "completed")
    job_filter = MatchFilter(
        job_filter, lambda j: match_string(j["state"], state), f"state is {state}"
    )
    if (choices := job_query.get("choose-from")) is not None:
        job_filter = ChooseFilter(
            LimitFilter(job_filter, choices),
            cache,
            "jobs" in args.again,
            extract_description=describe_job,
        )
    elif limit := job_query.get("limit-per-push"):
        job_filter = LimitFilter(job_filter, limit)

    return job_filter


def process_query(args, cache):
    if args.query == "default":
        query = DEFAULT_QUERY
    else:
        with open(args.query) as fh:
            query = yaml.safe_load(fh)

    job_query = query.get("jobs", {"choose-from": 0})
    job_filter = make_job_filter(job_query, cache)

    if local_artifacts := query.get("artifacts"):
        for push_id, a in enumerate(local_artifacts):
            push = {
                "id": push_id,
            }
            push_result = {
                "push": push,
                "push_id": push_id,
                "push_idx": push_id,
                "push_desc": f"local data {a}",
                "revision": "local",
                "repo": "local",
            }
            push_result["push_url"] = "file://" + a
            job = {
                "id": 0,
                "task_id": "__local__",
            }
            if "://" not in a:
                a = f"file://{a}"
            process_artifact(a, job, push_result, query, cache)
        return

    artifact_match = query["artifact"]

    pushes = resolve_pushes(query.get("pushes") or {"choose-from": 20}, cache)
    logger.info("Pushes: " + "+".join(str(p) for p in pushes))
    history = cache.value(("history", "pushes"), [])
    history.append(pushes)

    push_table = {}
    for push_id in pushes:
        push = get_push(push_id, cache)
        logger.info(f"Scanning push #{push['id']} {push['desc']}")

        push_result = {
            "push": push,
            "push_id": push_id,
            # Give each push an "index" set to an autoincrementing value associated with its push_id.
            "push_idx": push_table.setdefault(push_id, len(push_table)),
            "push_desc": summarize_push(push, cache),
            "revision": push["revision"],
            "repo": get_repository(push["repository_id"], cache),
        }
        push_result["push_url"] = PUSH_URL.format(**push_result)

        found = 0

        # Get the latest run if there are multiple runs, to ignore retried jobs.
        jobs = job_filter(get_jobs(push["id"]))
        for job in reversed(list(jobs)):
            found += 1
            for a in get_job_artifacts(job["id"], cache):
                if match_key(artifact_match, a):
                    process_artifact(a, job, push_result, query, cache)

        if found == 0:
            logger.warning(f"no jobs matching: '{job_filter.name}' for push {push['desc']}")


if args.record:
    os.makedirs(args.record, exist_ok=True)

# Ok, I am bad. The cache holds useful data collected during the run, so don't
# just set it to None if --no-cache is given. This is laziness; I ought to have
# a "real" data store and let the cache be a pure backing store.
cache = Cache(args.cache_root, args.refresh, args.no_cache)
cache.load()

if args.query:
    process_query(args, cache)
elif args.list_pushes:
    list_pushes(args)
elif args.list_jobs:
    if args.pushes is None:
        push = choose_push(cache, 20)
    else:
        push = resolve_pushes(None, cache, single=True)
    list_jobs(args, push, args.list_jobs, cache)
elif args.list_artifacts:
    list_artifacts(args, cache)
elif args.job:
    show_job(args)
else:
    args.query = "default"
    process_query(args, cache)

cache.save()
server.recorder.save()
