#!/usr/bin/python

import argparse
import gzip
import io
import json
import logging
import os
import re
import requests
import sys
import urllib
import yaml

from collections import defaultdict
from pathlib import Path
from typing import Any

DEFAULT_CACHE_ROOT = Path(os.environ.get("XDG_CACHE_HOME", os.path.expanduser("~/.cache")))
SITE = "https://treeherder.mozilla.org"
VERSION = 0.1
HEADERS = {"User-Agent": f"artifetch {VERSION} by sfink@mozilla.com"}

PUSH_URL = "https://treeherder.mozilla.org/jobs?repo={repo}&revision={revision}"
JOB_URL = "https://treeherder.mozilla.org/jobs?repo={repo}&revision={revision}&selectedTaskRun={task_id}"

logger = logging.getLogger("artifetch")
autoincrement = defaultdict(int)

if "--test" in sys.argv:
    iftest = lambda t: t
else:
    iftest = lambda t: argparse.SUPPRESS

parser = argparse.ArgumentParser(prog='artifetch', usage="artifetch [options]")
parser.add_argument("--version", action='version', version=f'%(prog)s {VERSION}')
parser.add_argument("--verbose", "-v", default=0, action="count", help="Verbose output")
parser.add_argument("--cache-root", default=str(DEFAULT_CACHE_ROOT), help="Root of cache dir, useful to override for testing")

parser.add_argument("--json", action="store_true", help="Dump out raw JSON output")
parser.add_argument("--output", "-o", help="File to save output to, default stdout")
parser.add_argument("--again", nargs="?", const="all", action="store",
                    help="Repeat last query")
parser.add_argument("--refresh", action="store_true", help="Do not use cached results")
parser.add_argument("--no-cache", action="store_true", help="Do not load from or save to cache")
parser.add_argument("--user", "-u", default=None, help="User to use for filtering pushes")
parser.add_argument("--branch", default="try",
                    help="The branch to use for commands that do not use a specific URL")

parser.add_argument("--query", help="Query YAML file")
parser.add_argument("--list-pushes",
                    nargs="?", const=10, action="store",
                    help="List out N most recent try pushes")
parser.add_argument("--list-jobs",
                    nargs="?", action="store", const=True,
                    help="List out jobs for a push (optional value, or use --push=PUSH)")
parser.add_argument("--list-artifacts", help="List out artifacts for a given JOB id")
parser.add_argument("--artifacts", help="List out artifacts matching ARTIFACT")
parser.add_argument("--show-job", dest="job", help="Display info for a given JOB id")

parser.add_argument("--push", help="Push ID to query")
parser.add_argument("--pushes", help="List of push IDs to query, or !n for the nth newest history item, counting from zero")

parser.add_argument("--test", action="store_true", help="enable extra options for testing")

parser.add_argument("--record", type=str, help=iftest("directory to record into"))
parser.add_argument("--replay", type=str, help=iftest("directory to replay from"))

args = parser.parse_args()

if args.verbose == 1:
    logger.setLevel(logging.INFO)
elif args.verbose > 1:
    logger.setLevel(logging.DEBUG)

loghandler = logging.StreamHandler()
loghandler.setFormatter(logging.Formatter("%(levelname)s - %(message)s"))
logger.addHandler(loghandler)

OutFile = sys.stdout
if args.output:
    OutFile = open(args.output, "wt")

if args.again == "all":
    args.again = ["pushes", "jobs"]
else:
    args.again = args.again.split(",")

class Cache(object):
# Anything here can be guaranteed to exist.
    EMPTY_CACHE = {
        "pushes": {
            # "spec=<spec>" for resolve_push_range a+b::c+d+e syntax (b::c ranges are looked up).
        },
        "job": {
            # <job_id>: { "artifacts": list of artifact urls }
        },
        "push": {
            # <push_id>: { "desc": description of push grabbed from first revision }
        },
    }

    def __init__(self, cache_root, refresh, no_cache):
        self.path = Path(cache_root) / "artifetch/jobs.json"
        self.artifact_path = Path(cache_root) / "artifetch/artifacts/"
        self.refresh = refresh
        self.no_cache = no_cache
        self.data = None

    def load(self):
        if self.refresh or self.no_cache:
            self.data = self.EMPTY_CACHE
            return

        try:
            with open(self.path, "rt") as fh:
                self.data = json.load(fh)
        except OSError:
            self.data = self.EMPTY_CACHE

    def save(self):
        if self.data is None:
            logger.info("no cache data to save")
            return
        if self.no_cache:
            logger.info("--no-cache given, not saving")
            return
        os.makedirs(os.path.dirname(self.path), exist_ok=True)
        with open(self.path, "wt") as fh:
            json.dump(self.data, fh)
        logger.info(f"saved to cache file {self.path}")

    def lookup(self, path):
        cache = self.data
        if cache is None:
            return None
        for p in path:
            if isinstance(cache, list):
                i = int(p)
                next = cache[i] if i < len(cache) else None
            else:
                next = cache.get(str(p))
            if next is None:
                return None
            cache = next
        logger.debug(f"loaded {'.'.join(str(p) for p in path)} from cache")
        return cache

    def value(self, path, value):
        '''Insert a value at a path in the cache, and return the value.'''
        cache = self.data
        if cache:
            for segment in path[:-1]:
                cache = cache.setdefault(str(segment), {})
            cache[path[-1]] = value
        return value

    def fetch_artifact(self, url):
        if url.startswith("file://"):
            # requests doesn't seem to handle file: URLs.
            filename = url[7:]
            return (Path(filename).read_text(), filename)

        cache_file_base = self.artifact_path / urllib.parse.quote(url, safe='')
        data = None
        for cache_file in (cache_file_base, Path(str(cache_file_base) + ".gz")):
            try:
                with gzip.open(cache_file.open("rb"), "rt") as fh:
                    data = fh.read()
            except gzip.BadGzipFile:
                data = cache_file.read_text()
            except OSError:
                pass

            if data is not None:
                break

        if data is not None and len(data):
            logger.info(f"used cached file {cache_file}")
            return (data, cache_file)

        # Hack: read full file into memory in order to handle both compressed and
        # uncompressed data.
        r = requests.get(url, stream=True)
        raw_data = r.raw.read()

        # Further hacks: the URL may not have a .gz extension even though the
        # data is compressed. Adjust the cache_file name to reflect the compression or lack thereof.

        self.artifact_path.mkdir(parents=True, exist_ok=True)

        try:
            data = gzip.GzipFile(fileobj=io.BytesIO(raw_data)).read().decode()
            if not str(cache_file).endswith(".gz"):
                cache_file = str(cache_file) + ".gz"
            cache_file.write_bytes(raw_data)
        except gzip.BadGzipFile:
            data = raw_data.decode()
            cache_file.write_text(data)

        return (data, cache_file)


def require_user():
    if args.user:
        return args.user
    import subprocess
    try:
        output = subprocess.check_output(["hg", "config", "ui.username"], text=True)
        # Extract just the email address from `My Name <myname@nowhere.com>`.
        if m := re.search(r'<(.*?)>', output):
            args.user = m.group(1)
        else:
            args.user = output.strip()
    except subprocess.CalledProcessError:
        output = subprocess.check_output(["git", "config", "--get", "user.email"], text=True)
        args.user = output.strip()

    return args.user


# I should be using thclient, but the pip-installed version is out of date, and
# the in-tree treeherder one is missing some endpoints that I need. And it
# doesn't really add much portability or anything.
def get(endpoint, project=None, **params):
    if project is not None:
        url = f"{SITE}/api/project/{project}/{endpoint}/"
    else:
        url = f"{SITE}/api/{endpoint}/"
    return requests.get(url, params=params, headers=HEADERS)


def get_results(endpoint, project=None, **params):
    if args.replay or args.record:
        autoincrement[endpoint] += 1
        filename = endpoint + "-" + str(autoincrement[endpoint])

    if args.replay:
        text = (Path(args.replay) / filename).read_text()
        data = json.loads(text)
    else:
        response = get(endpoint, project=project, **params)
        if not response.ok:
            logger.error(f"API request failed: {response.status_code} {response.reason}")
            logger.error(response.text)
            sys.exit(1)
        data = response.json()

    if args.record:
        (Path(args.record) / filename).write_text(json.dumps(data))

    return data["results"] if endpoint != "repository" else data

def graphql(task_id):
    headers = HEADERS
    headers.update({"Content-Type": "application/json"})
    response = requests.post(
        "https://firefox-ci-tc.services.mozilla.com/graphql",
        json={
            "operationName": "Task",
            "variables": {
                "taskId": task_id,
                "artifactsConnection": {
                    "limit": 1000
                },
            },
            "query": """\
query Task(
    $taskId: ID!,
    $artifactsConnection: PageConnection
) {
  task(taskId: $taskId) {
    taskId
    taskGroupId
    routes
    extra
    metadata {
      name
      description
      owner
      __typename
    }
    status {
      state
      runs {
        taskId
        runId
        state
        artifacts(connection: $artifactsConnection) {
          ...Artifacts
          __typename
        }
        __typename
      }
      __typename
    }
    __typename
  }
}

fragment Artifacts on ArtifactsConnection {
  pageInfo {
    hasNextPage
    hasPreviousPage
    cursor
    previousCursor
    nextCursor
    __typename
  }
  edges {
    node {
      name
      contentType
      __typename
    }
    __typename
  }
  __typename
}"""
        },
        headers=headers
    )

    data = response.json()
    if errors := data.get("errors"):
        for error in errors:
            logger.error("{code}: {message}".format(**error))
        raise Exception(",".join(e["code"] for e in errors))
    return data


# Global variable because I am bad.
PushDescriptions = {}


# Summarize and cache a push.
def summarize_push(push, cache):
    cache_path = ("push", push["id"])
    if desc := cache.lookup(cache_path + ("desc",)):
        return desc

    rev = push["revisions"][0]
    desc = re.sub(r'\n+\s*', ' ; ', rev["comments"])
    if len(desc) >= 76:
        desc = desc[0:76] + "..."
    PushDescriptions[push["id"]] = desc
    push["desc"] = desc
    cache.value(cache_path, push)
    return desc


def list_pushes(args):
    user = require_user()
    pushes = get_results("push", project=args.branch, author=user, count=args.list_pushes)
    if args.json:
        print(json.dumps(pushes, indent=4), file=OutFile)
        return

    for p in pushes:
        print(f"{p['id']} - {len(p['revisions'])} revs", file=OutFile)
        for rev in p["revisions"]:
            desc = re.sub(r'\n.*', '', rev["comments"])
            print(f"  {rev['revision']} {desc}", file=OutFile)


def get_push_summary(push_id, cache):
    if desc := cache.lookup(("push", push_id, "desc")):
        return desc
    push = get_results("push", project=args.branch, id=push_id)[0]
    return summarize_push(push, cache)


def get_push(push_id, cache):
    if push := cache.lookup(("push", push_id)):
        return push
    push = get_results("push", project=args.branch, id=push_id)[0]
    summarize_push(push, cache)
    return push


def choose_pushes(cache, n=10):
    import pyfzf
    fzf = pyfzf.FzfPrompt()
    user = require_user()
    pushes = get_results("push", project=args.branch,
                         author=user, count=n)
    options = [f"{p['id']} - {summarize_push(p, cache)}" for p in pushes]
    choices = fzf.prompt(options, "--multi --no-sort --marker=* --ansi")
    return [c[0:c.index(" ")] for c in choices]


def get_jobs(push_id):
    return get_results("jobs", project=args.branch, push_id=push_id, count=1000)


def list_jobs(args):
    if not args.push:
        raise Exception("--push ID required")

    jobs = get_jobs(args.push)

    if args.json:
        print(json.dumps(jobs, indent=4), file=OutFile)
        return

    for j in jobs:
        group = j["job_group_symbol"]
        symbol = j["job_type_symbol"]
        job_symbol = symbol if group == "?" else f"{group}({symbol})"
        print("  {id} - {job_symbol} ({state}: {result}) task {task_id}".format(
            **j, job_symbol=job_symbol), file=OutFile)
        print("    {job_type_name} - {job_guid}".format(**j), file=OutFile)


def job_symbol(job, cache):
    if job is None:
        return "?"
    if job["job_group_symbol"] == "?":
        return job["job_type_symbol"]
    return "{job_group_symbol}({job_type_symbol})".format(**job)


def describe_job(job_id, cache):
    job = cache.lookup(("job", job_id))
    return job_symbol(job, cache)


def get_job_artifacts(job_id, cache=None):
    if cjob := cache.lookup(("job", job_id)):
        if artifacts := cjob.get("artifacts"):
            if "job_group_symbol" in cjob:
                logger.info(f"loaded job {job_id} {describe_job(job_id, cache)} from cache: {len(artifacts)} artifacts")
                return artifacts

    job = get_results('jobs', project=args.branch, id=job_id)[-1]

    for k in ("job_group_symbol", "job_group_name", "job_type_name", "job_type_symbol", "result", "state"):
        cache.value(("job", job_id, k), job.get(k))

    task = job["task_id"]
    data = graphql(task)["data"]
    runs = data["task"]["status"]["runs"]
    lastrun = runs[-1]
    edges = lastrun["artifacts"]["edges"]
    names = [e["node"]["name"] for e in edges]
    artifacts = []
    for name in names:
        artifacts.append("{site}/api/queue/v1/task/{task}/runs/{run}/artifacts/{artifact}".format(
            site="https://firefox-ci-tc.services.mozilla.com",
            task=task,
            run=lastrun["runId"],
            artifact=urllib.parse.quote(name, safe=''),
        ))
    cache.value(("job", job_id, "artifacts"), artifacts)
    logger.debug(f"job {job_id} {describe_job(job_id, cache)}: {len(artifacts)} artifacts")
    return artifacts


def list_artifacts(args):
    for a in get_job_artifacts(args.list_artifacts):
        print(a, file=OutFile)


def list_matching_artifacts(args):
    match = args.artifacts
    push = args.push
    for job in get_jobs(push):
        logger.debug(f"Fetching artifacts for push {push} job {job['id']}")
        for a in get_job_artifacts(job["id"]):
            if match in a:
                print(a, file=OutFile)


def show_job(args):
    if not args.job:
        raise Exception("--job ID required")

    job = get_results('jobs', project=args.branch, id=args.job)

    if args.json:
        print(json.dumps(job, indent=4), file=OutFile)


def get_pushes(start, end):
    result = []

    user = require_user()
    pushes = get_results("push", project=args.branch, author=user, count=50)
    for p in reversed(pushes):
        id = p["id"]
        if id == start:
            result.append(id)
        elif len(result):
            result.append(id)
            if id == end:
                return result

    recent = [p["id"] for p in pushes]
    raise Exception(f"do not see {start}::{end} in recent pushes {recent}")


def get_pushes_matching(filter):
    user = require_user()
    pushes = get_results("push", project=args.branch, author=user, count=20)
    return [p["id"] for p in reversed(pushes) if filter(p)]


def resolve_push_range(spec, cache):
    cache_path = ("pushes", f"spec={spec}")
    if pushes := cache.lookup(cache_path):
        return pushes

    pushes = []
    for part in spec.split("+"):
        range = part.split("::")
        if len(range) == 1:
            pushes.append(part)
        else:
            pushes.extend(get_pushes(int(range[0]), int(range[1])))

    return cache.value(cache_path, pushes)


def resolve_pushes(spec, cache):
    if args.pushes:
        if args.pushes.startswith("!"):
            spec = {"ids": "+".join(cache.lookup(("history", "pushes", -int(args.pushes[1:]))))}
        else:
            spec = {"ids": args.pushes}

    if ids := spec.get("ids"):
        return resolve_push_range(ids, cache)

    if comment := spec.get("comment"):
        # Do not cache, because we may be rerunning to pick up newer pushes.
        def push_has_comment(p):
            return any([comment in r["comments"] for r in p["revisions"]])
        return get_pushes_matching(filter=push_has_comment)

    if revision := spec.get("rev"):
        # Do not cache, because we may be rerunning to pick up newer pushes.
        def rev_matches(rev, pattern):
            return rev in pattern.split("+")
        def push_has_rev(p):
            return any([rev_matches(r["revision"], revision) for r in p["revisions"]])
        return get_pushes_matching(filter=push_has_rev)

    if nchoices := spec.get("choose-from"):
        return choose_pushes(cache, nchoices)

    raise Exception("pushes request must be 'ids' or 'comment'")


DataT = dict[str, Any] | list[Any]
PathT = list[str | int]
ResultPathsT = list[PathT]


def parse_path(k : str) -> PathT:
    return [p for p in re.findall(r'\[\]|[^.\[]+|\.', k) if p != '.']


def find_path_matches(data : DataT, path : PathT, sofar : PathT=[]) -> list[tuple[PathT, Any]]:
    if not path:
        return [(sofar, data)]
    results = []
    if path[0] == '[]':
        for i in range(len(data)):
            results += find_path_matches(data[i], path[1:], sofar + [i])
    else:
        k = path[0]
        if isinstance(data, list):
            k = int(k)
        results += find_path_matches(data[k], path[1:], sofar + [k])
    return results


def resolve_path(path : PathT, bases : ResultPathsT):
    result = []
    for segment in path:
        if segment.startswith("$"):
            i = int(segment[1:] or '1')
            # $n converts to the *parent* of match-key-n.
            result.extend(bases[i][:-1])
        elif segment == "__parent__":
            result.pop()
        else:
            result.append(segment)
    return result


def lookup(path : PathT, data):
    for segment in path:
        data = data[segment]
    return data


def lookup_json_values_g(extractor, match : ResultPathsT, data, labels):
    path = parse_path(extractor["values"])
    path = resolve_path(path, match)

    for p, datum in find_path_matches(data, path):
        result = {"value": datum}
        for lname, lpath in labels:
            result[lname] = lookup(resolve_path(lpath, match), data)
        yield result


def lookup_text_values_g(extractor, data, labels):
    pattern = extractor["keys"][1]
    assert(pattern.startswith("/") and pattern.endswith("/"))
    matcher = re.compile(pattern[1:-1])
    for line in data.splitlines():
        if m := re.search(matcher, line):
            result = {}
            for lname, lvalue in [("value", extractor["values"])] + labels:
                result[lname] = re.sub(r'\$(\d+)', lambda lm: m.group(int(lm.group(1))), lvalue)
            yield result

def lookup_values_g(extractor, match, data, labels):
    if extractor["type"] == "json":
        yield from lookup_json_values_g(extractor, match, data, labels)
    elif extractor["type"] == "text":
        yield from lookup_text_values_g(extractor, data, labels)
    elif extractor["type"] == "files":
        yield {}


def match_key(key : str, val):
    if key.startswith("/"):
        return bool(re.search(key[1:-1], val))
    else:
        val = re.sub(r'.*%2F', "", val)
        return key == val


PrevPush = None


def get_repository(repo_id, cache):
    repo = cache.lookup(("repository", repo_id))
    if repo:
        return repo

    results = get_results("repository")
    cached = cache.value(("repository"), {})
    retrieved = {str(r["id"]): r["name"] for r in results}
    cached.update(retrieved)
    return cached[str(repo_id)]


def output_metric(result, added, output={}, cache=None):
    global PrevPush
    global PushDescriptions

    style = output.get("style", "formatted")
    format = output.get("format", "{push_idx} {value}")
    header_format = output.get("job-header", "# {desc}" if "desc" in result else False)

    if style not in ("gnuplot", "formatted"):
        raise Exception(f"unsupported output format {output}")

    if header_format != False:
        if result["push_id"] != PrevPush:
            if PrevPush is not None:
                print("", file=OutFile)
            print(header_format.format(**result), file=OutFile)
        PrevPush = result["push_id"]

    # Every time a new index is generated, output the label-header format so the index can be mapped to the data it corresponds to.
    for label, idx, label_value in added:
        if label_format := output.get("label-header", {}).get(label):
            print(label_format.format(**result), file=OutFile)

    print(format.format(**result), file=OutFile)


def parse_json_metric_extractor(metric):
    keys = [None]
    targets = [None]

    # TODO: Error checking. (This assumes the keys are well-named and
    # corresponding.)

    json = metric["json"]

    if mk1 := json.get("match-key"):
        keys.append(parse_path(mk1))
        targets.append(json["match-value"])
    else:
        for k in json.keys():
            parts = k.split("match-key-", 1)
            if len(parts) == 1:
                continue
            i = int(parts[1])
            while len(keys) <= i:
                keys.append(None)
                targets.append(None)
            keys[i] = parse_path(json[f"match-key-{i}"])
            targets[i] = json[f"match-value-{i}"]

    return {
        "type": "json",
        "keys": keys,
        "targets": targets,
        "values": json["value"],
        "labels": json.get("label"),
    }


def parse_text_metric_extractor(metric):
    spec = metric["text"]
    return {
        "type": "text",
        "keys": (None, spec["match-key"],),
        "targets": (None, True,),
        "values": spec["value"],
        "labels": spec.get("label", ()),
    }


def parse_metric_extractor(metric):
    if "json" in metric:
        return parse_json_metric_extractor(metric)
    if "text" in metric:
        return parse_text_metric_extractor(metric)
    return {
        "type": "files",
        "labels": {},
        "keys": (None,),
        "targets": (None,),
    }


def match_remaining(matchers, paths: ResultPathsT, data : DataT) -> list[ResultPathsT]:
    if not matchers:
        return [paths]

    i, keypath, target =  matchers[0]
    keypath = resolve_path(keypath, paths)

    result : list[ResultPathsT] = []
    for path, keyval in find_path_matches(data, keypath):
        if match_key(target, keyval):
            result.append(paths + [path])
    return result


def extract_matches_g(extractor, data : DataT):
    matchers = list(zip(
        range(len(extractor["keys"])),
        extractor["keys"],
        extractor["targets"]
    ))
    matchers.pop(0)  # Matchers are 1-based indexes. Sorry.

    matches : list[ResultPathsT] = []

    if extractor["type"] == "files":
        yield from lookup_values_g(extractor, None, data, {})
        return

    # For each match for the first match-key
    matcher = matchers.pop(0)

    if extractor["type"] == "json":
        labels = []
        for label, path in (extractor["labels"] or {}).items():
            labels.append((label, parse_path(path)))

        for path, keyval in find_path_matches(data, matcher[1]):
            if match_key(matcher[2], keyval):
                # Find everything that matches the rest.
                matches += match_remaining(matchers, [None, path], data)

        for match in matches:
            yield from lookup_values_g(extractor, match, data, labels)

    else:
        labels = list((extractor["labels"] or {}).items())
        yield from lookup_values_g(extractor, None, data, labels)


class GroupBy(object):
    def __init__(self, groupby, expr, cache):
        self.groupby = groupby
        self.expr = expr
        self.cache = cache
        self.aggregates = []
        for m in re.finditer(r'{(\w+)\((\w+)\)}', expr):
            self.aggregates.append((m.group(1), m.group(2)))
        self.sums = defaultdict(lambda: defaultdict(int))
        self.counts = defaultdict(int)
        self.proto_result = {}

        self.job = {}
        self.filename = {}
        self.output = None

    def __call__(self, result, added, output, cache):
        rawkey = (result[k] for k in self.groupby)
        # Allow `groupby: ["push"]`` for example (objects will be distinguished by their "id" values.)
        key = tuple(k["id"] if isinstance(k, dict) else k for k in rawkey)
        for func, field in self.aggregates:
            self.sums[key][field] += int(result[field])
            self.counts[key] += 1
        self.proto_result.setdefault(key, {}).update(result)

    def output_results(self, output):
        for key in self.sums.keys():
            # This is a complete mess.
            result = dict(self.proto_result[key])
            for m in re.finditer(r'{(\w+)\((\w+)\)}', self.expr):
                func, field = m.groups()
                call = m.group(0)[1:-1]
                if func == "sum":
                    result[call] = self.sums[key][field]
                elif func == "count":
                    result[call] = self.counts[key]
                elif func == "mean":
                    result[call] = self.sums[key][field] / self.counts[key]
                else:
                    raise Exception(f"unknown aggregate function '{func}'")

            output_metric(result, (), output, self.cache)


def process_artifact(url, job, push_result, query, cache):
    logger.info(f"process artifact {url}")
    raw, filename = cache.fetch_artifact(url)
    metric = query["metric"]
    extractor = parse_metric_extractor(metric)
    if extractor["type"] == "json":
        data : DataT = json.loads(raw)
    elif extractor["type"] == "text":
        data : DataT = raw
    else:
        data : DataT = None
    output = metric.get("output", {})

    if groupby := output.get("groupby"):
        handler = GroupBy(groupby, output["format"], cache)
    else:
        handler = output_metric

    # {label name: {value: idx}}
    labeled_values = defaultdict(dict)

    job_result = dict(push_result)
    job_result.update({
        "filename": filename,
        "job": job,
        "job_id": job["id"],
        "job_desc": describe_job(job["id"], cache),
        "task_id": job["task_id"],
    })
    job_result["job_url"] = JOB_URL.format(**job_result)

    indexes = {}
    for result in extract_matches_g(extractor, data):
        result.update(job_result)
        added = []
        for label, value in result.items():
            if isinstance(value, dict):
                # eg job will be skipped. job_idx will be based off of job_id.
                continue
            if label.endswith("_idx"):
                # don't need indexes of indexes
                continue
            if label.endswith("_id"):
                # eg job_id -> job, which will then generate job_idx
                label = label[:-3]

            if f"{label}_idx" in result:
                continue  # eg push_idx is a global index; do not override.
            map = labeled_values[label]
            if (idx := map.get(value)) is None:
                idx = len(map)
                added.append((label, idx, value))
                map[value] = idx

            indexes[f"{label}_idx"] = idx

        result.update(indexes)
        handler(result, added, output, cache)

    if output.get("groupby"):
        handler.output_results(output)


class BaseFilter(object):
    def __init__(self, base_filter = None, desc = None):
        self.base = base_filter
        if desc is None:
            self.name = base_filter.name if base_filter else "all items"
        else:
            self.name = f"all {desc}"

    def __call__(self, incoming):
        if self.base is None:
            return incoming
        return self.base(incoming)


class LimitFilter(BaseFilter):
    def __init__(self, base_filter, limit):
        super().__init__(base_filter)
        self.limit = limit
        if self.limit:
            self.name = f"first {self.limit} of {base_filter.name}"
        else:
            self.name = base_filter.name

    def __call__(self, incoming):
        results = []
        for item in self.base(incoming):
            results.append(item)
            if self.limit and len(results) >= limit:
                return results
        return results


class MatchFilter(BaseFilter):
    def __init__(self, base_filter, matcher, desc):
        super().__init__(base_filter)
        self.name = f"({base_filter.name} with {desc}"
        self.matcher = matcher

    def __call__(self, incoming):
        return filter(self.match, incoming)


class ChooseFilter(BaseFilter):
    def __init__(self, base_filter, cache, again, extract_description=lambda s: s):
        super().__init__(base_filter)
        self.extract_description = extract_description
        self.cache = cache
        self.again = again
        setattr(ChooseFilter, "count", 0)

    def __call__(self, incoming):
        incoming = list(incoming)  # flatten iterator

        ChooseFilter.count += 1
        n = ChooseFilter.count
        # Ugh.
        cache_path = ("history", "choose", f"choose-{n}")

        indexes = None
        if self.again:
            previous = self.cache.lookup(cache_path)
            if previous is not None:
                indexes = [int(i) for i in previous]

        if indexes is None:
            import pyfzf
            fzf = pyfzf.FzfPrompt()
            descriptions = map(self.extract_description, incoming)
            ids = range(1, len(incoming) + 1)
            options = [f"{id} - {desc}" for id, desc in zip(ids, descriptions)]
            choices = fzf.prompt(options, "--multi --no-sort --marker=* --ansi")
            indexes = [int(c[0:c.index(" ")]) - 1 for c in choices]
            self.cache.value(cache_path, indexes)

        return [incoming[i] for i in indexes]


def process_query(args, cache):
    if args.query == "default":
        query = {
            "artifact": "live_backing.log",
            "metric": {
                "output": {
                    "style": "formatted",
                    "format": "{filename}",
                }
            }}
    else:
        with open(args.query) as fh:
            query = yaml.safe_load(fh)

    job_filter = BaseFilter(None, "jobs")

    if local_artifacts := query.get("artifacts"):
        for push_id, a in enumerate(local_artifacts):
            push = {
                "id": push_id,
            }
            push_result = {
                "push": push,
                "push_id": push_id,
                "push_idx": push_id,
                "push_desc": f"local data {a}",
                "revision": "local",
                "repo": "local",
            }
            push_result["push_url"] = "file://" + a
            job = {
                "id": 0,
                "task_id": "__local__",
            }
            if "://" not in a:
                a = f"file://{a}"
            process_artifact(a, job, push_result, query, cache)
        return

    artifact_match = query["artifact"]

    job_query = query.get("jobs", {"choose-from": 0})
    if sym := job_query.get("symbol"):
        job_filter = MatchFilter(job_filter, lambda j: job_symbol(j, cache) == sym, f"symbol is {sym}")
    if (choices := job_query.get("choose-from")) is not None:
        job_filter = ChooseFilter(LimitFilter(job_filter, choices),
                                  cache,
                                  "jobs" in args.again,
                                  lambda j: job_symbol(j, cache))
    elif limit := job_query.get("limit-per-push"):
        job_filter = LimitFilter(job_filter, limit)

    pushes = resolve_pushes(query.get("pushes") or {"choose-from": 20}, cache)
    logger.info("Pushes: " + "+".join(str(p) for p in pushes))
    history = cache.value(("history", "pushes"), [])
    history.append(pushes)

    push_table = {}
    for push_id in pushes:
        push = get_push(push_id, cache)
        logger.info(f"Scanning push {push['desc']}")

        push_result = {
            "push": push,
            "push_id": push_id,
            # Give each push an "index" set to an autoincrementing value associated with its push_id.
            "push_idx": push_table.setdefault(push_id, len(push_table)),
            "push_desc": summarize_push(push, cache),
            "revision": push['revision'],
            "repo": get_repository(push['repository_id'], cache),
        }
        push_result["push_url"] = PUSH_URL.format(**push_result)

        for job in get_jobs(push["id"]):
            logger.debug(f"checking job {job_symbol(job, cache)} against {job_filter.name}")
        found = 0
        for job in job_filter(reversed(get_jobs(push["id"]))):
            found += 1
            for a in get_job_artifacts(job["id"], cache):
                if match_key(artifact_match, a):
                    process_artifact(a, job, push_result, query, cache)

        if found == 0:
            logger.warning(f"no jobs matching: '{job_filter.name}' for push {push['desc']}")


if args.record:
    os.makedirs(args.record, exist_ok=True)

# Ok, I am bad. The cache holds useful data collected during the run, so don't
# just set it to None if --no-cache is given. This is laziness; I ought to have
# a "real" data store and let the cache be a pure backing store.
cache = Cache(args.cache_root, args.refresh, args.no_cache)
cache.load()

if args.query:
    process_query(args, cache)
elif args.list_pushes:
    list_pushes(args)
elif args.list_jobs:
    if not isinstance(args.list_jobs, bool):
        args.push = args.list_jobs
    list_jobs(args)
elif args.list_artifacts:
    list_artifacts(args)
elif args.artifacts:
    list_matching_artifacts(args)
elif args.job:
    show_job(args)
else:
    print(json.dumps(choose_pushes(cache, 20)))
    print("Do what, now?")

cache.save()
