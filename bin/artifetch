#!/usr/bin/python

import argparse
import gzip
import io
import json
import logging
import os
import re
import requests
import sys
import urllib
import yaml

from collections import defaultdict
from typing import Any

def join_cache_path(path):
    root_cache_dir = os.environ.get("XDG_CACHE_HOME", os.path.expanduser("~/.cache"))
    return os.path.join(root_cache_dir, path)


CACHE = join_cache_path("artifetch/jobs.json")
ARTIFACT_CACHE = join_cache_path("artifetch/artifacts/")
SITE = "https://treeherder.mozilla.org"
VERSION = 0.1
HEADERS = {"User-Agent": f"artifetch {VERSION} by sfink@mozilla.com"}

# Anything here can be guaranteed to exist.
EMPTY_CACHE = {
    "pushes": {
        # "spec=<spec>" for resolve_push_range a+b::c+d+e syntax (b::c ranges are looked up).
    },
    "job": {
        # <job_id>: { "artifacts": list of artifact urls }
    },
    "push": {
        # <push_id>: { "desc": description of push grabbed from first revision }
    },
}

parser = argparse.ArgumentParser(prog='artifetch', usage="artifetch [options]")
parser.add_argument("--version", action='version', version=f'%(prog)s {VERSION}')
parser.add_argument("--verbose", "-v", default=0, action="count", help="Verbose output")

parser.add_argument("--json", action="store_true", help="Dump out raw JSON output")
parser.add_argument("--output", "-o", help="File to save output to, default stdout")
parser.add_argument("--refresh", action="store_true", help="Do not use cached results")
parser.add_argument("--no-cache", action="store_true", help="Do not use or save to cache")
parser.add_argument("--user", "-u", default=None, help="User to use for filtering pushes")
parser.add_argument("--branch", default="try",
                    help="The branch to use for commands that do not use a specific URL")

parser.add_argument("--query", help="Query YAML file")
parser.add_argument("--list-pushes",
                    nargs="?", const=10, action="store",
                    help="List out N most recent try pushes")
parser.add_argument("--list-jobs",
                    nargs="?", action="store", const=True,
                    help="List out jobs for a push (optional value, or use --push=PUSH)")
parser.add_argument("--list-artifacts", help="List out artifacts for a given JOB id")
parser.add_argument("--artifacts", help="List out artifacts matching ARTIFACT")
parser.add_argument("--show-job", dest="job", help="Display info for a given JOB id")

parser.add_argument("--push", help="Push ID to query")
parser.add_argument("--pushes", help="List of push IDs to query, or !n for the nth newest history item, counting from zero")


def load_cache(path):
    if args.refresh:
        return EMPTY_CACHE

    try:
        with open(path, "rt") as fh:
            return json.load(fh)
    except OSError:
        return EMPTY_CACHE


def save_cache(path, data):
    if data is None:
        logging.info(f"no cache data to save")
        return
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "wt") as fh:
        json.dump(data, fh)
    logging.info(f"saved to cache file {path}")


def cache_lookup(cache, path):
    if cache is None:
        return None
    for p in path:
        if isinstance(cache, list):
            i = int(p)
            next = cache[i] if i < len(cache) else None
        else:
            next = cache.get(str(p))
        if next is None:
            return None
        cache = next
    logging.debug(f"loaded {'.'.join(str(p) for p in path)} from cache")
    return cache


def cache_value(cache, path, value):
    if cache:
        for segment in path[:-1]:
            cache = cache.setdefault(str(segment), {})
        cache[path[-1]] = value
    return value


def require_user():
    if args.user:
        return args.user
    import subprocess
    try:
        output = subprocess.check_output(["hg", "config", "ui.username"], text=True)
        # Extract just the email address from `My Name <myname@nowhere.com>`.
        if m := re.search(r'<(.*?)>', output):
            args.user = m.group(1)
        else:
            args.user = output.strip()
    except subprocess.CalledProcessError:
        output = subprocess.check_output(["git", "config", "--get", "user.email"], text=True)
        args.user = output.strip()

    return args.user


# I should be using thclient, but the pip-installed version is out of date, and
# the in-tree treeherder one is missing some endpoints that I need. And it
# doesn't really add much portability or anything.
def get(endpoint, project=None, **params):
    if project is not None:
        url = f"{SITE}/api/project/{project}/{endpoint}/"
    else:
        url = f"{SITE}/api/{endpoint}/"
    return requests.get(url, params=params, headers=HEADERS)


def get_results(endpoint, project=None, **params):
    response = get(endpoint, project=project, **params)
    if response.ok:
        return response.json()["results"]
    logging.error("API request failed: {response.status_code} {response.reason}")
    logging.error(response.text)
    sys.exit(1)


def graphql(task_id):
    headers = HEADERS
    headers.update({"Content-Type": "application/json"})
    return requests.post(
        "https://firefox-ci-tc.services.mozilla.com/graphql",
        json={
            "operationName":"Task",
            "variables":{
                "taskId":task_id,
                "artifactsConnection":{
                    "limit":1000
                },
            },
            "query":"""\
query Task(
    $taskId: ID!,
    $artifactsConnection: PageConnection
) {
  task(taskId: $taskId) {
    taskId
    taskGroupId
    routes
    extra
    metadata {
      name
      description
      owner
      __typename
    }
    status {
      state
      runs {
        taskId
        runId
        state
        artifacts(connection: $artifactsConnection) {
          ...Artifacts
          __typename
        }
        __typename
      }
      __typename
    }
    __typename
  }
}

fragment Artifacts on ArtifactsConnection {
  pageInfo {
    hasNextPage
    hasPreviousPage
    cursor
    previousCursor
    nextCursor
    __typename
  }
  edges {
    node {
      name
      contentType
      __typename
    }
    __typename
  }
  __typename
}"""
        },
        headers=headers
    )

# Global variable because I am bad.
PushDescriptions = {}


# Summarize and cache a push.
def summarize_push(push, cache):
    cache_path = ("push", push["id"])
    if desc := cache_lookup(cache, cache_path + ("desc",)):
        return desc

    rev = push["revisions"][0]
    desc = re.sub(r'\n+\s*', ' ; ', rev["comments"])
    if len(desc) >= 76:
        desc = desc[0:76] + "..."
    PushDescriptions[push["id"]] = desc
    push["desc"] = desc
    cache_value(cache, cache_path, push)
    return desc


def list_pushes(args):
    user = require_user()
    pushes = get_results("push", project=args.branch, author=user, count=args.list_pushes)
    if args.json:
        print(json.dumps(pushes, indent=4), file=OutFile)
        return

    for p in pushes:
        print(f"{p['id']} - {len(p['revisions'])} revs", file=OutFile)
        for rev in p["revisions"]:
            desc = re.sub(r'\n.*', '', rev["comments"])
            print(f"  {rev['revision']} {desc}", file=OutFile)


def get_push_summary(push_id, cache):
    if desc := cache_lookup(cache, ("push", push_id, "desc")):
        return desc
    push = get_results("push", project=args.branch, id=push_id)[0]
    return summarize_push(push, cache)


def get_push(push_id, cache):
    if push := cache_lookup(cache, ("push", push_id)):
        return push
    push = get_results("push", project=args.branch, id=push_id)[0]
    summarize_push(push, cache)
    return push


def choose_pushes(cache, n=10):
    import pyfzf
    fzf = pyfzf.FzfPrompt()
    user = require_user()
    pushes = get_results("push", project=args.branch,
                         author=user, count=n)
    options = [f"{p['id']} - {summarize_push(p, cache)}" for p in pushes]
    choices = fzf.prompt(options, "--multi --no-sort --marker=* --ansi")
    return [c[0:c.index(" ")] for c in choices]


def get_jobs(push_id):
    return get_results("jobs", project=args.branch, push_id=push_id, count=1000)


def list_jobs(args):
    if not args.push:
        raise Exception("--push ID required")

    jobs = get_jobs(args.push)

    if args.json:
        print(json.dumps(jobs, indent=4), file=OutFile)
        return

    for j in jobs:
        group = j["job_group_symbol"]
        symbol = j["job_type_symbol"]
        job_symbol = symbol if group == "?" else f"{group}({symbol})"
        print("  {id} - {job_symbol} ({state}: {result}) task {task_id}".format(
            **j, job_symbol=job_symbol), file=OutFile)
        print("    {job_type_name} - {job_guid}".format(**j), file=OutFile)


def job_symbol(job, cache):
    if job is None:
        return "?"
    if job["job_group_symbol"] == "?":
        return job["job_type_symbol"]
    return "{job_group_symbol}({job_type_symbol})".format(**job)


def describe_job(job_id, cache):
    job = cache_lookup(cache, ("job", job_id))
    return job_symbol(job, cache)


def get_job_artifacts(job_id, cache=None):
    if cjob := cache_lookup(cache, ("job", job_id)):
        if artifacts := cjob.get("artifacts"):
            if "job_group_symbol" in cjob:
                logging.info(f"loaded job {job_id} {describe_job(job_id, cache)} from cache: {len(artifacts)} artifacts")
                return artifacts

    job = get_results('jobs', project=args.branch, id=job_id)[-1]

    for k in ("job_group_symbol", "job_group_name", "job_type_name", "job_type_symbol", "result", "state"):
        cache_value(cache, ("job", job_id, k), job.get(k))

    task = job["task_id"]
    data = graphql(task).json()["data"]
    runs = data["task"]["status"]["runs"]
    lastrun = runs[-1]
    state = lastrun["state"]
    edges = lastrun["artifacts"]["edges"]
    names = [e["node"]["name"] for e in edges]
    artifacts = []
    for name in names:
        artifacts.append("{site}/api/queue/v1/task/{task}/runs/{run}/artifacts/{artifact}".format(
            site="https://firefox-ci-tc.services.mozilla.com",
            task=task,
            run=lastrun["runId"],
            artifact=urllib.parse.quote(name, safe=''),
        ))
    cache_value(cache, ("job", job_id, "artifacts"), artifacts)
    logging.debug(f"job {job_id} {describe_job(job_id, cache)}: {len(artifacts)} artifacts")
    return artifacts


def list_artifacts(args):
    for a in get_job_artifacts(args.list_artifacts):
        print(a, file=OutFile)


def list_matching_artifacts(args):
    match = args.artifacts
    push = args.push
    for job in get_jobs(push):
        logging.debug(f"Fetching artifacts for push {push} job {job['id']}")
        for a in get_job_artifacts(job["id"]):
            if match in a:
                print(a, file=OutFile)


def show_job(args):
    if not args.job:
        raise Exception("--job ID required")

    job = get_results('jobs', project=args.branch, id=args.job)

    if args.json:
        print(json.dumps(job, indent=4), file=OutFile)


def get_pushes(start, end):
    result = []

    user = require_user()
    pushes = get_results("push", project=args.branch, author=user, count=20)
    for p in reversed(pushes):
        id = p["id"]
        if id == start:
            result.append(id)
        elif len(result):
            result.append(id)
            if id == end:
                return result

    recent = [p["id"] for p in pushes]
    raise Exception(f"do not see {start}::{end} in recent pushes {recent}")


def get_pushes_matching(filter):
    user = require_user()
    pushes = get_results("push", project=args.branch, author=user, count=20)
    return [p["id"] for p in reversed(pushes) if filter(p)]


def resolve_push_range(spec, cache):
    cache_path = ("pushes", f"spec={spec}")
    if pushes := cache_lookup(cache, cache_path):
        return pushes

    pushes = []
    for part in spec.split("+"):
        range = part.split("::")
        if len(range) == 1:
            pushes.append(part)
        else:
            pushes.extend(get_pushes(int(range[0]), int(range[1])))

    return cache_value(cache, cache_path, pushes)


def resolve_pushes(spec, cache):
    if args.pushes:
        if args.pushes.startswith("!"):
            spec = {"ids": "+".join(cache_lookup(cache, ("history", "pushes", -int(args.pushes[1:]))))}
        else:
            spec = {"ids": args.pushes}

    if ids := spec.get("ids"):
        return resolve_push_range(ids, cache)

    if comment := spec.get("comment"):
        # Do not cache, because we may be rerunning to pick up newer pushes.
        def push_has_comment(p):
            return any([comment in r["comments"] for r in p["revisions"]])
        return get_pushes_matching(filter=push_has_comment)

    if revision := spec.get("rev"):
        # Do not cache, because we may be rerunning to pick up newer pushes.
        def rev_matches(rev, pattern):
            return rev in pattern.split("+")
        def push_has_rev(p):
            return any([rev_matches(r["revision"], revision) for r in p["revisions"]])
        return get_pushes_matching(filter=push_has_rev)

    if nchoices := spec.get("choose-from"):
        return choose_pushes(cache, nchoices)

    raise Exception("pushes request must be 'ids' or 'comment'")


def fetch_artifact(url):
    cache_file_base = os.path.join(ARTIFACT_CACHE, urllib.parse.quote(url, safe=''))
    data = None
    for cache_file in (cache_file_base, cache_file_base + ".gz"):
        try:
            with gzip.open(open(cache_file, "rb"), "rt") as fh:
                data = fh.read()
        except gzip.BadGzipFile:
            with open(cache_file, "rt") as fh:
                data = fh.read()
        except OSError:
            pass

        if data is not None:
            break

    if data is not None and len(data):
        logging.info(f"used cached file {cache_file}")
        return (data, cache_file)

    # Hack: read full file into memory in order to handle both compressed and
    # uncompressed data.
    r = requests.get(url, stream=True)
    raw_data = r.raw.read()

    # Further hacks: the URL may not have a .gz extension even though the
    # data is compressed. Adjust the cache_file name to reflect the compression or lack thereof.

    os.makedirs(ARTIFACT_CACHE, exist_ok=True)

    try:
        data = gzip.GzipFile(fileobj=io.BytesIO(raw_data)).read().decode()
        if not cache_file.endswith(".gz"):
            cache_file += ".gz"
        with open(cache_file, "wb") as fh:
            fh.write(raw_data)
    except gzip.BadGzipFile:
        data = raw_data.decode()
        with open(cache_file, "wt") as fh:
            fh.write(data)

    return (data, cache_file)


Data = dict[str, Any] | list[Any]
Path = list[str | int]
ResultPaths = list[Path]


def parse_path(k : str) -> Path:
    return [p for p in re.findall(r'\[\]|[^.\[]+|\.', k) if p != '.']


def find_path_matches(data : Data, path : Path, sofar : Path=[]) -> list[tuple[Path, Any]]:
    import pdb; pdb.set_trace()
    if not path:
        return [(sofar, data)]
    results = []
    if path[0] == '[]':
        for i in range(len(data)):
            results += find_path_matches(data[i], path[1:], sofar + [i])
    else:
        k = path[0]
        if isinstance(data, list):
            k = int(k)
        results += find_path_matches(data[k], path[1:], sofar + [k])
    return results


def resolve_path(path : Path, bases : ResultPaths):
    result = []
    for segment in path:
        if segment.startswith("$"):
            i = int(segment[1:] or '1')
            # $n converts to the *parent* of match-key-n.
            result.extend(bases[i][:-1])
        elif segment == "__parent__":
            result.pop()
        else:
            result.append(segment)
    return result


def lookup(path : Path, data):
    for segment in path:
        data = data[segment]
    return data


def lookup_json_values_g(extractor, match : ResultPaths, data, labels):
    path = parse_path(extractor["values"])
    path = resolve_path(path, match)

    for p, datum in find_path_matches(data, path):
        result = {"value": datum}
        for lname, lpath in labels:
            result[lname] = lookup(resolve_path(lpath, match), data)
        yield result


def lookup_text_values_g(extractor, data, labels):
    pattern = extractor["keys"][1]
    assert(pattern.startswith("/") and pattern.endswith("/"))
    matcher = re.compile(pattern[1:-1])
    for line in data.splitlines():
        if m := re.search(matcher, line):
            result = {}
            for lname, lvalue in [("value", extractor["values"])] + labels:
                result[lname] = re.sub(r'\$(\d+)', lambda lm: m.group(int(lm.group(1))), lvalue)
            yield result

def lookup_values_g(extractor, match, data, labels):
    if extractor["type"] == "json":
        yield from lookup_json_values_g(extractor, match, data, labels)
    elif extractor["type"] == "text":
        yield from lookup_text_values_g(extractor, data, labels)


def match_key(key : str, val):
    if key.startswith("/"):
        return bool(re.search(key[1:-1], val))
    else:
        val = re.sub(r'.*%2F', "", val)
        return key == val


PushTable : dict[int, int] = {}
PrevPush = None

def number(v, table):
    if (n := table.get(v)) is not None:
        return n
    n = len(table)
    table[v] = n
    return n


# FIXME
def guess_repository(repo_id, cache):
    if repo_id == 4:
        return 'try'
    return 'mozilla-central'


def output_metric(push, job, result, added, filename, output={}, cache=None):
    global PrevPush
    global PushDescriptions

    style = output.get("style", "gnuplot")
    format = output.get("format", "{push_idx} {value}")
    header_format = output.get("header-format", "# {desc}")

    if style != "gnuplot":
        raise Exception(f"unsupported output format {output}")

    # Warning: all locals are accessible in the varous output format strings.
    # So the naming matters.

    push_id = int(push["id"])
    push_desc = summarize_push(push, cache)
    repo = guess_repository(push['repository_id'], cache)
    revision = push['revision']
    push_url = f"https://treeherder.mozilla.org/jobs?repo={repo}&revision={revision}"
    job_id = job["id"]
    job_desc = describe_job(job_id, cache)
    task_id = job['task_id']
    job_url = f"https://treeherder.mozilla.org/jobs?repo={repo}&revision={revision}&selectedTaskRun={task_id}"

    if header_format != False:
        if push_id != PrevPush:
            if PrevPush is not None:
                print("", file=OutFile)
            print(header_format.format(**locals()), file=OutFile)
        PrevPush = push_id

    #if result['process_idx'] == 1 and result['mempath'] == "js-main-runtime/gc-heap/unused-arenas":
    #    import pdb; pdb.set_trace()

    push_idx = number(push_id, PushTable)
    for label, idx, label_value in added:
        if label_format := output.get("label-format", {}).get(label):
            print(label_format.format(**locals(), **result), file=OutFile)

    print(format.format(**locals(), **result), file=OutFile)


def parse_json_metric_extractor(metric):
    keys = [None]
    targets = [None]

    # TODO: Error checking. (This assumes the keys are well-named and
    # corresponding.)

    json = metric["json"]

    if mk1 := json.get("match-key"):
        keys.append(parse_path(mk1))
        targets.append(json["match-value"])
    else:
        for k in json.keys():
            parts = k.split("match-key-", 1)
            if len(parts) == 1:
                continue
            i = int(parts[1])
            while len(keys) <= i:
                keys.append(None)
                targets.append(None)
            keys[i] = parse_path(json[f"match-key-{i}"])
            targets[i] = json[f"match-value-{i}"]

    return {
        "type": "json",
        "keys": keys,
        "targets": targets,
        "values": json["value"],
        "labels": json.get("label"),
    }


def parse_text_metric_extractor(metric):
    spec = metric["text"]
    return {
        "type": "text",
        "keys": (None, spec["match-key"],),
        "targets": (None, True,),
        "values": spec["value"],
        "labels": spec.get("label", ()),
    }


def parse_metric_extractor(metric):
    if "json" in metric:
        return parse_json_metric_extractor(metric)
    if "text" in metric:
        return parse_text_metric_extractor(metric)
    raise Exception("no metric spec found")


def match_remaining(matchers, paths: ResultPaths, data : Data) -> list[ResultPaths]:
    if not matchers:
        return [paths]

    i, keypath, target =  matchers[0]
    keypath = resolve_path(keypath, paths)

    result : list[ResultPaths] = []
    for path, keyval in find_path_matches(data, keypath):
        if match_key(target, keyval):
            result.append(paths + [path])
    return result


def extract_matches_g(extractor, data : Data):
    matchers = list(zip(
        range(len(extractor["keys"])),
        extractor["keys"],
        extractor["targets"]
    ))
    matchers.pop(0)  # Matchers are 1-based indexes. Sorry.

    matches : list[ResultPaths] = []

    # For each match for the first match-key
    matcher = matchers.pop(0)

    if extractor["type"] == "json":
        labels = []
        for label, path in (extractor["labels"] or {}).items():
            labels.append((label, parse_path(path)))

        for path, keyval in find_path_matches(data, matcher[1]):
            if match_key(matcher[2], keyval):
                # Find everything that matches the rest.
                matches += match_remaining(matchers, [None, path], data)

        for match in matches:
            for result in lookup_values_g(extractor, match, data, labels):
                yield result

    elif extractor["type"] == "text":
        labels = list((extractor["labels"] or {}).items())
        yield from lookup_values_g(extractor, None, data, labels)


def process_artifact(url, job, push, query, cache):
    logging.info(f"process artifact {url}")
    raw, filename = fetch_artifact(url)
    metric = query["metric"]
    extractor = parse_metric_extractor(metric)
    if extractor["type"] == "json":
        data : Data = json.loads(raw)
    else:
        data : Data = raw
    output = metric.get("output", {})

    # {label name: {value: idx}}
    labeled_values = defaultdict(dict)

    indexes = {}
    for result in extract_matches_g(extractor, data):
        added = []
        for label, value in result.items():
            map = labeled_values[label]
            if (idx := map.get(value)) is None:
                idx = len(map)
                added.append((label, idx, value))
                map[value] = idx
            indexes[label + "_idx"] = idx
        result.update(indexes)
        output_metric(push, job, result, added, filename, output, cache)


def limited_filter(job_filter, limit, filter_state):
    def new_filter(job):
        if filter_state["njobs"] >= limit or not job_filter(job):
            return False
        filter_state["njobs"] += 1
        return True
    return new_filter


def process_query(args, cache):
    with open(args.query) as fh:
        query = yaml.safe_load(fh)
    artifact_match = query["artifact"]

    filter_name = "all jobs"
    job_filter = lambda j: True
    filter_state = {"njobs": 0}
    if job_query := query.get("jobs"):
        if sym := job_query.get("symbol"):
            job_filter = lambda j: job_symbol(j, cache) == sym
            filter_name = f"jobs matching {sym}"
        if limit := job_query.get("limit-per-push"):
            filter_state["job_limit"] = limit
            job_filter = limited_filter(job_filter, limit, filter_state)
            filter_name = f"first of {filter_name}"

    pushes = resolve_pushes(query.get("pushes", {"choose-from": 20}), cache)
    logging.info("Pushes: " + "+".join(pushes))
    if cache:
        history = cache.setdefault("history", {})
        history.setdefault("pushes", []).append(pushes)

    for push_id in pushes:
        push = get_push(push_id, cache)
        logging.info(f"Scanning push {push['desc']}")

        filter_state["njobs"] = 0
        for job in get_jobs(push["id"]):
            logging.debug(f"checking job {job_symbol(job, cache)} against {filter_name}")
        for job in filter(job_filter, reversed(get_jobs(push["id"]))):
            for a in get_job_artifacts(job["id"], cache):
                if match_key(artifact_match, a):
                    process_artifact(a, job, push, query, cache)

        if filter_state["njobs"] == 0:
            logging.warning(f"no jobs matching: '{filter_name}' for push {push['desc']}")


args = parser.parse_args()

if args.verbose == 1:
    logging.basicConfig(level=logging.INFO)
elif args.verbose > 1:
    logging.basicConfig(level=logging.DEBUG)

OutFile = sys.stdout
if args.output:
    OutFile = open(args.output, "wt")

# Ok, I am bad. The cache holds useful data collected during the run, so don't
# just set it to None if --no-cache is given. This is laziness; I ought to have
# a "real" data store and let the cache be a pure backing store.
cache = EMPTY_CACHE
if not args.no_cache:
    cache = load_cache(CACHE)

if args.query:
    process_query(args, cache)
elif args.list_pushes:
    list_pushes(args)
elif args.list_jobs:
    if not isinstance(args.list_jobs, bool):
        args.push = args.list_jobs
    list_jobs(args)
elif args.list_artifacts:
    list_artifacts(args)
elif args.artifacts:
    list_matching_artifacts(args)
elif args.job:
    show_job(args)
else:
    print(json.dumps(choose_pushes(cache, 20)))
    print("Do what, now?")

if not args.no_cache:
    save_cache(CACHE, cache)
